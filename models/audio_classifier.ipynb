{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c6cef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.optim import Optimizer\n",
    "\n",
    "def add_noise(features, noise_level=0.01):\n",
    "    return features + noise_level * torch.randn_like(features)\n",
    "\n",
    "def calculate_multilabel_accuracy(outputs, labels, threshold=0.5):\n",
    "    predictions = (torch.sigmoid(outputs) > threshold).float()\n",
    "    correct_predictions = (predictions == labels).float()\n",
    "    accuracy = correct_predictions.mean().item()\n",
    "    return accuracy\n",
    "\n",
    "# Train model (w/ grad)\n",
    "def train(model: nn.Module, train_dataloader: DataLoader, test_dataloader: DataLoader, optimizer: Optimizer, loss_fn, batch_size, num_epochs=20, print_every=10, patience=5):\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "\n",
    "    train_accuracies = []\n",
    "    test_accuracies = []\n",
    "\n",
    "    best_val_accuracy = 0\n",
    "    best_model_state = None\n",
    "    patience_counter = 0\n",
    "    \n",
    "    for ep in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        running_acc = 0.0\n",
    "        num_batches = 0\n",
    "\n",
    "        for features, labels in train_dataloader:\n",
    "            features = add_noise(features, noise_level=0.01)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(features)\n",
    "            loss = loss_fn(outputs, labels.float())\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            # Calculate accuracy for multilabel\n",
    "            running_acc += calculate_multilabel_accuracy(outputs, labels)\n",
    "            num_batches += 1\n",
    "            \n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "        train_loss = running_loss / num_batches\n",
    "        train_acc = running_acc / num_batches\n",
    "        train_losses.append(train_loss)\n",
    "        train_accuracies.append(train_acc)\n",
    "\n",
    "        val_loss, val_accuracy = evaluate(model, test_dataloader, loss_fn)\n",
    "        test_losses.append(val_loss)\n",
    "        test_accuracies.append(val_accuracy)\n",
    "\n",
    "        if val_accuracy > best_val_accuracy:\n",
    "            best_val_accuracy = val_accuracy\n",
    "            best_model_state = model.state_dict()\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"Early stopping at epoch {ep}\")\n",
    "                break\n",
    "\n",
    "        if print_every > 0 and ep % print_every == 0:\n",
    "            print(f\"Epoch: {ep} | Train Loss: {train_loss:.4f} | Train Accuracy: {train_acc:.4f} | \"\n",
    "                  f\"Test Loss: {val_loss:.4f} | Test Accuracy: {val_accuracy:.4f}\")\n",
    "            \n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "\n",
    "    return train_losses, test_losses, train_accuracies, test_accuracies\n",
    "\n",
    "\n",
    "# Evaluate model (no grad)\n",
    "def evaluate(model: nn.Module, dataloader: DataLoader, loss_fn):\n",
    "    total_loss = 0\n",
    "    total_accuracy = 0\n",
    "    num_batches = 0\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for features, labels in dataloader:\n",
    "            outputs = model(features)\n",
    "            loss = loss_fn(outputs, labels.float())\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # Calculate accuracy for multilabel\n",
    "            total_accuracy += calculate_multilabel_accuracy(outputs, labels)\n",
    "            num_batches += 1\n",
    "\n",
    "    avg_loss = total_loss / num_batches\n",
    "    avg_accuracy = total_accuracy / num_batches\n",
    "\n",
    "    return avg_loss, avg_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c499b1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VGGish multilabel classification model - input a VGGish 10x128 feature -> output multiple binary predictions\n",
    "# Each output represents presence/absence of a particular audio class\n",
    "\n",
    "class VGGishNet(nn.Module):\n",
    "    def __init__(self, hidden_dim=64, dropout=0.2, num_classes=8):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(128, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.BatchNorm1d(hidden_dim // 2),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim // 2, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.mean(dim=1) # Average across time dimension (batch_size, time, features) -> (batch_size, features)\n",
    "        return self.net(x)\n",
    "    \n",
    "# Load the data\n",
    "train_features = torch.load('../data/train_features.pt')\n",
    "train_labels = torch.load('../data/train_labels.pt')\n",
    "\n",
    "test_features = torch.load('../data/test_features.pt')\n",
    "test_labels = torch.load('../data/test_labels.pt')\n",
    "\n",
    "custom_features = torch.load('../data/custom_test_features.pt')\n",
    "custom_labels = torch.load('../data/custom_test_labels.pt')\n",
    "\n",
    "# Calculate mean and std from training data\n",
    "train_mean = train_features.mean(dim=0, keepdim=True)\n",
    "train_std = train_features.std(dim=0, keepdim=True)\n",
    "\n",
    "# Normalize all datasets using training statistics\n",
    "train_features = (train_features - train_mean) / (train_std + 1e-7)\n",
    "test_features = (test_features - train_mean) / (train_std + 1e-7)\n",
    "custom_features = (custom_features - train_mean) / (train_std + 1e-7)\n",
    "\n",
    "train_dataset = TensorDataset(train_features, train_labels)\n",
    "# test_dataset = TensorDataset(torch.vstack([test_features, custom_features]), torch.cat([test_labels, custom_labels]))\n",
    "test_dataset = TensorDataset(test_features, test_labels)\n",
    "custom_dataset = TensorDataset(custom_features, custom_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2493c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "hidden_sizes = [16, 32, 64, 128]\n",
    "dropouts = np.linspace(0, 0.5, 6)\n",
    "weight_decays = np.logspace(-5, 0, 6)\n",
    "learning_rates = np.logspace(-5, -1, 5)\n",
    "batch_sizes = [8, 16, 32, 64, 128]\n",
    "\n",
    "best_accuracy = 0.0\n",
    "best_model = None\n",
    "best_params = None\n",
    "best_train_losses = []\n",
    "best_test_losses = []\n",
    "best_train_accuracies = []\n",
    "best_test_accuracies = []\n",
    "acc_params = []\n",
    "\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "for bs in batch_sizes:\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=int(bs), shuffle=True, drop_last=True)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=int(bs), shuffle=False, drop_last=False)\n",
    "    \n",
    "    for i in range(10):\n",
    "        hs = int(np.random.choice(hidden_sizes))\n",
    "        do = float(np.random.choice(dropouts))\n",
    "        wd = float(np.random.choice(weight_decays))\n",
    "        lr = float(np.random.choice(learning_rates))\n",
    "\n",
    "        model = VGGishNet(hidden_dim=hs, dropout=do)\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=wd)\n",
    "        train_losses, test_losses, train_accuracies, test_accuracies = train(model, train_dataloader, test_dataloader, optimizer, loss_fn, bs, num_epochs=50, print_every=0, patience=20)\n",
    "        \n",
    "        final_accuracy = evaluate(model, test_dataloader, loss_fn)[1]\n",
    "        \n",
    "        if best_model is None or final_accuracy > best_accuracy:\n",
    "            best_model = VGGishNet(hidden_dim=hs, dropout=do)\n",
    "            best_model.load_state_dict(model.state_dict())\n",
    "            best_accuracy = final_accuracy\n",
    "            best_params = (hs, do, wd, lr, bs)\n",
    "            best_train_losses = train_losses\n",
    "            best_test_losses = test_losses\n",
    "            best_train_accuracies = train_accuracies\n",
    "            best_test_accuracies = test_accuracies\n",
    "\n",
    "        acc_params.append((final_accuracy, hs, do, wd, lr, bs))\n",
    "\n",
    "        print(f\"hs: {hs}, do: {do}, wd: {wd}, lr: {lr}, bs: {bs}, train acc: {np.max(train_accuracies)}, test acc: {final_accuracy}\")\n",
    "          \n",
    "acc_params.sort(key=lambda x: x[0], reverse=True)\n",
    "print(acc_params[:5])\n",
    "\n",
    "print(\"Best params (hidden size, dropout, weight decay, learning rate, batch size):\", best_params, best_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a552b5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(best_train_losses, label='Train Loss')\n",
    "plt.plot(best_test_losses, label='Test Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Epochs vs. Loss')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(best_train_accuracies, label='Train Accuracy')\n",
    "plt.plot(best_test_accuracies, label='Test Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.title('Epochs vs. Accuracy')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5414fe8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('train (loss, accuracy)', evaluate(best_model, train_dataloader, nn.CrossEntropyLoss()))\n",
    "print('test:', evaluate(best_model, DataLoader(TensorDataset(test_features, test_labels)), nn.CrossEntropyLoss()))\n",
    "print('custom:', evaluate(best_model, DataLoader(TensorDataset(custom_features, custom_labels)), nn.CrossEntropyLoss()))\n",
    "print('combined (test+custom):', evaluate(best_model, test_dataloader, nn.CrossEntropyLoss()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "1bfd4b09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10792"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = VGGishNet()\n",
    "model.load_state_dict(torch.load('audio-classifier-model-weights.pth', map_location='cpu'))\n",
    "\n",
    "sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
