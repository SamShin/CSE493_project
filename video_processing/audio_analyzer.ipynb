{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628caccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from yt_dlp import YoutubeDL\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from os import path\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.optim import Optimizer\n",
    "\n",
    "# Retrieve the video links from json file\n",
    "def getVideoLinks() -> list[str]:\n",
    "    file_path = path.join(path.dirname(os.getcwd()), 'data/videos.json')\n",
    "    with open(file_path) as f:\n",
    "        data = json.load(f)\n",
    "        return [video['link'] for video in data]\n",
    "    \n",
    "\n",
    "# Retrieve the downloaded file names\n",
    "def getFileNames() -> list[str]:\n",
    "    return os.listdir(path.join(path.dirname(os.getcwd()), 'data/dl_audios'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1141ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run when audios aren't downloaded - get the video links and download audios locally to data/dl_audios\n",
    "\n",
    "URLs = getVideoLinks()\n",
    "\n",
    "file_path = path.join(path.dirname(os.getcwd()), 'data/dl_audios')\n",
    "ydl_config = {\n",
    "    'format': 'm4a/bestaudio/best',\n",
    "    'postprocessors': [{\n",
    "        'key': 'FFmpegExtractAudio',\n",
    "        'preferredcodec': 'wav',\n",
    "    }],\n",
    "    'outtmpl': path.join(file_path, '%(title)s.%(ext)s'),\n",
    "}\n",
    "\n",
    "with YoutubeDL(ydl_config) as ydl:\n",
    "    error_code = ydl.download(URLs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57471e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessAudio(y: np.ndarray) -> np.ndarray:\n",
    "    y = librosa.to_mono(y) # set to mono\n",
    "    y = y / np.abs(y).max() # normalize amplitude\n",
    "    return y\n",
    "\n",
    "# Get the log mels of the files\n",
    "def getLogMels(file_names: list[str]) -> np.ndarray:\n",
    "    log_mels = np.ndarray(len(file_names), dtype=np.ndarray)\n",
    "\n",
    "    for i, fn in enumerate(file_names):\n",
    "        fp = path.join(path.join(path.dirname(os.getcwd()), 'data/dl_audios'), fn)\n",
    "        y, sr = librosa.load(fp, sr=sr_param)\n",
    "        y = preprocessAudio(y)\n",
    "        mel = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=128)\n",
    "        log_mels[i] = librosa.power_to_db(mel, ref=np.max)\n",
    "\n",
    "    return log_mels\n",
    "\n",
    "sr_param = 16000\n",
    "file_names = getFileNames()\n",
    "log_mels = getLogMels(file_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f422b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize first 3 log mel spectrograms\n",
    "\n",
    "for i in range(3):\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    librosa.display.specshow(log_mels[i], sr=sr_param, x_axis='time', y_axis='mel')\n",
    "    plt.colorbar(format='%+2.0f dB')\n",
    "    plt.title(f'Log-Mel Spectrogram: {file_names[i]}')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d5b9bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training data (clips of crying/laughing babies)\n",
    "\n",
    "# data_files = ['../data/train_features.pt', '../data/test_features.pt', '../data/train_labels.pt', '../data/test_labels.pt']\n",
    "\n",
    "data_files = ['../data/train_features.pt', '../data/test_features.pt', '../data/train_labels.pt', '../data/test_labels.pt']\n",
    "\n",
    "if not all(os.path.exists(f) for f in data_files):\n",
    "    \n",
    "    class_labels = {\n",
    "        'cry': (23, '/t/dd00002'),\n",
    "        'laugh': (17, '/t/dd00001'),\n",
    "        'music': (137, '/m/04rlf'),\n",
    "        'singing': (27, '/m/015lz1'),\n",
    "        'child_speech': (3, '/m/0ytgt'),\n",
    "        'male_speech': (1, '/m/05zppz'),\n",
    "        'female_speech': (2, '/m/02zsn'),\n",
    "        'lullaby': (271, '/m/07pkxdp'),\n",
    "    }\n",
    "\n",
    "    df_train_bal = pd.read_csv(\n",
    "        '../data/audioset_segments/balanced_train_segments.csv',\n",
    "        comment='#',\n",
    "        header=None,\n",
    "        names=['YTID', 'start_seconds', 'end_seconds', 'labels'],\n",
    "        sep = r'\\s+',\n",
    "        engine='python',\n",
    "    )\n",
    "\n",
    "    df_train_unbal = pd.read_csv(\n",
    "        '../data/audioset_segments/unbalanced_train_segments.csv',\n",
    "        comment='#',\n",
    "        header=None,\n",
    "        names=['YTID', 'start_seconds', 'end_seconds', 'labels'],\n",
    "        sep = r'\\s+',\n",
    "        engine='python',\n",
    "    )\n",
    "\n",
    "    df_test = pd.read_csv(\n",
    "        '../data/audioset_segments/eval_segments.csv',\n",
    "        comment='#',\n",
    "        header=None,\n",
    "        names=['YTID', 'start_seconds', 'end_seconds', 'labels'],\n",
    "        sep = r'\\s+',\n",
    "        engine='python',\n",
    "    )\n",
    "\n",
    "    bal = {}\n",
    "    unbal = {}\n",
    "    test = {}\n",
    "\n",
    "    for label, (class_idx, class_code) in class_labels.items():\n",
    "        bal[label] = df_train_bal[df_train_bal['labels'].str.contains(class_code)]\n",
    "        unbal[label] = df_train_unbal[df_train_unbal['labels'].str.contains(class_code)]\n",
    "        test[label] = df_test[df_test['labels'].str.contains(class_code)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f273192",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !! Run script in scripts/audioset_features_downloader.py before running\n",
    "# Load the VGG-like extracted features from the dataset\n",
    "\n",
    "context_features = {\n",
    "    'video_id': tf.io.FixedLenFeature([], dtype=tf.string),\n",
    "    'start_time_seconds': tf.io.FixedLenFeature([], dtype=tf.float32),\n",
    "    'end_time_seconds': tf.io.FixedLenFeature([], dtype=tf.float32),\n",
    "    'labels': tf.io.VarLenFeature(dtype=tf.int64),\n",
    "}\n",
    "\n",
    "sequence_features = {\n",
    "    \"audio_embedding\": tf.io.FixedLenSequenceFeature([], dtype=tf.string)\n",
    "}\n",
    "\n",
    "# Parse the AudioSet sequence\n",
    "def parseSquence(serialized_example):\n",
    "    context, sequence = tf.io.parse_single_sequence_example(\n",
    "        serialized=serialized_example,\n",
    "        context_features=context_features,\n",
    "        sequence_features=sequence_features\n",
    "    )\n",
    "\n",
    "    # Decode the 128-dimensional 8-bit quantized audio embedding bytes - (1, 128) vector per second\n",
    "    audio_embeddings = tf.map_fn(\n",
    "        lambda x: tf.io.decode_raw(x, tf.uint8),\n",
    "        sequence[\"audio_embedding\"],\n",
    "        dtype=tf.uint8\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"video_id\": context[\"video_id\"],\n",
    "        \"start_time\": context[\"start_time_seconds\"],\n",
    "        \"end_time\": context[\"end_time_seconds\"],\n",
    "        \"labels\": tf.sparse.to_dense(context[\"labels\"]),\n",
    "        \"audio_embedding\": audio_embeddings,\n",
    "    }\n",
    "\n",
    "\n",
    "class_id_to_index = {label[0]: idx for idx, label in enumerate(class_labels.values())}\n",
    "\n",
    "# Function to convert label tensor to binary vector\n",
    "def process_labels(label_tensor):\n",
    "    # Initialize a binary vector of 8 zeros (one for each class)\n",
    "    binary_vector = [0] * len(class_labels)\n",
    "    \n",
    "    # Loop through each class ID in the label tensor and set the corresponding index to 1\n",
    "    for class_id in label_tensor.numpy():\n",
    "        index = class_id_to_index.get(class_id, None)\n",
    "        if index is not None:\n",
    "            binary_vector[index] = 1\n",
    "            \n",
    "    return binary_vector\n",
    "\n",
    "\n",
    "# Retrieve the 128-D features from the sequences - produces a list of PyTorch tensors\n",
    "def get_features_and_labels(dataset: pd.DataFrame, isTrain: bool, bal = False):\n",
    "    features = []\n",
    "    labels = []\n",
    "\n",
    "    for id in dataset['YTID']:\n",
    "        split = 'bal_train' if isTrain and bal else ('unbal_train' if isTrain else 'eval')\n",
    "        rd = tf.data.TFRecordDataset(f\"../data/audioset_v1_embeddings/{split}/{id[:2]}.tfrecord\")\n",
    "        rd = rd.map(parseSquence)\n",
    "        \n",
    "        if len(features) > 300:\n",
    "            return features, labels\n",
    "        \n",
    "        for sample in rd:\n",
    "            vid_id = sample['video_id'].numpy().decode(\"utf-8\")\n",
    "\n",
    "            if vid_id == id[:len(id) - 1]:\n",
    "                features.append(torch.from_numpy(sample[\"audio_embedding\"].numpy()))\n",
    "                labels.append(process_labels(sample['labels']))\n",
    "                break\n",
    "\n",
    "    return features, labels\n",
    "\n",
    "\n",
    "if not all(os.path.exists(f) for f in data_files):\n",
    "    \n",
    "    train_features = []\n",
    "    test_features = []\n",
    "\n",
    "    train_labels = []\n",
    "    test_labels = []\n",
    "\n",
    "    for label, (class_idx, class_code) in class_labels.items():\n",
    "        print(f\"---------- {label}\")\n",
    "        train_features_bal, train_labels_bal = get_features_and_labels(bal[label], isTrain=True, bal=True)\n",
    "        if label == \"music\" or label == \"singing\":\n",
    "            train_features_unbal = []\n",
    "            train_labels_unbal = []\n",
    "        else:\n",
    "            train_features_unbal, train_labels_unbal = get_features_and_labels(unbal[label], isTrain=True)\n",
    "\n",
    "        train_features_class = (train_features_bal + train_features_unbal)[:300]\n",
    "        train_labels_class = (train_labels_bal + train_labels_unbal)[:300]\n",
    "\n",
    "        test_features_class, test_labels_class = get_features_and_labels(test[label], isTrain=False)\n",
    "\n",
    "        train_features += train_features_class\n",
    "        train_labels += train_labels_class\n",
    "\n",
    "        test_features += test_features_class[:50]\n",
    "        test_labels += test_labels_class[:50]\n",
    "\n",
    "    # Padded features to 10 secs\n",
    "    train_features = pad_sequence(train_features, batch_first=True)\n",
    "    test_features = pad_sequence(test_features, batch_first=True)\n",
    "\n",
    "    torch.save(train_features.float(), '../data/train_features.pt')\n",
    "    torch.save(test_features.float(), '../data/test_features.pt')\n",
    "    torch.save(torch.Tensor(train_labels).float(), '../data/train_labels.pt')\n",
    "    torch.save(torch.Tensor(test_labels).float(), '../data/test_labels.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
